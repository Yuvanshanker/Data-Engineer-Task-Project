{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aOP6yreo7tOM"
      },
      "outputs": [],
      "source": [
        "import cv2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab.patches import cv2_imshow"
      ],
      "metadata": {
        "id": "n-LCgPM075nI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Object Detection\n",
        "\n",
        "net = cv2.dnn.readNet(\"yolov3.weights\", \"yolov3.cfg\")\n",
        "\n",
        "# Load COCO labels\n",
        "classes = []\n",
        "with open(\"coco.names\", \"r\") as f:\n",
        "    classes = f.read().strip().split(\"\\n\")\n",
        "\n",
        "# Load video\n",
        "cap = cv2.VideoCapture(\"Copy of market.mp4\")\n",
        "\n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    height, width, _ = frame.shape\n",
        "\n",
        "    # Prepare input image for YOLO\n",
        "    blob = cv2.dnn.blobFromImage(frame, 1/255, (416, 416), swapRB=True, crop=False)\n",
        "    net.setInput(blob)\n",
        "\n",
        "    # Get detections\n",
        "    outs = net.forward(net.getUnconnectedOutLayersNames())\n",
        "\n",
        "    # Process detections\n",
        "    # ... (drawing bounding boxes)\n",
        "\n",
        "    # Display output frame\n",
        "    cv2.imwrite(\"Frame.jpg\",frame)\n",
        "\n",
        "\n",
        "    # display the image in the notebook\n",
        "    display(Image(filename=\"Frame.jpg\"))\n",
        "\n",
        "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
        "        break\n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()\n"
      ],
      "metadata": {
        "id": "_ncxln-w75wa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pose Estimation\n",
        "\n",
        "import cv2\n",
        "net = cv2.dnn.readNetFromCaffe(\"pose_deploy.prototxt\")\n",
        "\n",
        "# Load video\n",
        "cap = cv2.VideoCapture(\"Copy of market.mp4\")\n",
        "\n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    height, width, _ = frame.shape\n",
        "\n",
        "    # Prepare input image for OpenPose\n",
        "    blob = cv2.dnn.blobFromImage(frame, 1.0, (368, 368), (127.5, 127.5, 127.5), swapRB=False, crop=False)\n",
        "    net.setInput(blob)\n",
        "\n",
        "    # Get keypoints\n",
        "    output = net.forward()\n",
        "\n",
        "    # Process keypoints\n",
        "    # ... (drawing keypoints)\n",
        "\n",
        "    # Display output frame\n",
        "    cv2.imshow(\"Frame\", frame)\n",
        "\n",
        "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
        "        break\n",
        "\n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "X8SU2nuO75yy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FbK2VD7b8bSR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Action Recognition\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "\n",
        "\n",
        "# Load your action sequences dataset and labels\n",
        "X = np.load(height)  # Shape: (num_samples, sequence_length, num_features)\n",
        "y = np.load(width)      # Shape: (num_samples, num_classes)\n",
        "\n",
        "# Split the dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define LSTM model\n",
        "model = Sequential([\n",
        "    LSTM(64, input_shape=(sequence_length, num_features)),\n",
        "    Dense(num_classes, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Test loss: {loss:.4f}, Test accuracy: {accuracy:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "42mSMNPx751o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FmNYqrm6755B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Fpe_TL7r76Cf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}